{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maste\\envs\\envtest\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from doc import Doc, standardizeWord, standardizeChar\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import model classes\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency data\n",
    "freqFile = open('frequencyList.txt', \"r\", encoding=\"utf8\")\n",
    "freqList = freqFile.read()\n",
    "freqFile.close()\n",
    "freqLines = freqList.split(\"\\n\")\n",
    "frequencies = {}\n",
    "for line in freqLines:\n",
    "    if (line != ''):\n",
    "        data = line.split(\":::\")\n",
    "        lemma = standardizeWord(data[1])\n",
    "        pos = data[2]\n",
    "        freq = data[0]\n",
    "        frequencies[lemma+\":::\"+pos] = freq\n",
    "freqSeries = pd.Series(frequencies, dtype='int')\n",
    "freqSeries = freqSeries.sort_values(ascending=False)\n",
    "#print(freqSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99% processed\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#create the data file\n",
    "dir = 'C:\\\\Users\\\\maste\\\\Desktop\\\\RAFT\\\\arabicScraperOutput'\n",
    "docs = {}\n",
    "fout = open(\"arabicReading.data\", \"w+\",encoding='utf8')\n",
    "fileCount = 0\n",
    "for filename in os.listdir(dir):\n",
    "    numFiles =  numFiles = len(os.listdir(dir))\n",
    "    if os.path.isfile(dir+\"\\\\frequency_list\\\\\"+filename):\n",
    "        doc = Doc(dir,filename)\n",
    "        docs[filename]=doc\n",
    "        df = doc.frequencyTable\n",
    "        for line in doc.lemmaList:\n",
    "            count = doc.lemmaList[line]\n",
    "            key = doc.getKeyFromLemmaListLine(line)\n",
    "            if key != '':\n",
    "                p = key.split(\":::\")\n",
    "                lemma = p[0]\n",
    "                pos = p[1]\n",
    "                if key != '' and key in freqSeries.index:\n",
    "                    freq = freqSeries.index.get_loc(key)+1\n",
    "                    for i in range(0,count):\n",
    "                        ndf = pd.DataFrame([[lemma,pos,freq]],columns=['lemma','pos','freq']);\n",
    "                        df = df.append(ndf)\n",
    "        df.freq = df.freq.astype('int')\n",
    "        doc.frequencyTable = df\n",
    "        doc.getFrequencyInfo()\n",
    "        #print(len(doc.lemmaList))\n",
    "        #print(doc.frequencyTable.sort_values(by=['freq'],ascending=False))\n",
    "        #print(filename,doc.getDataString())\n",
    "        sys.stdout.write(\"\\r %i%% processed\" % (fileCount/numFiles * 100))\n",
    "        sys.stdout.flush()\n",
    "        fout.write(doc.getDataString()+\"\\n\")\n",
    "    fileCount += 1\n",
    "fout.close()\n",
    "print('')\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the pandas data frame\n",
    "names = ['word count','sentence count','sentence length','lemma ratio','p95','mean','median','noun','verb','prep','part','conj','adv','adj','difficulty','word length']\n",
    "data = pd.DataFrame(columns = names)\n",
    "for name in docs:\n",
    "    d = docs[name]\n",
    "    TESTDATA = StringIO(d.getDataString())\n",
    "    df = pd.read_csv(TESTDATA,header=None,names=names,dtype='float64')\n",
    "    data=data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init variables for machine learning\n",
    "array = data.values\n",
    "len_names = len(names)\n",
    "features = array[:,0:len_names-1]  # comma in slice signifies a tuple (tuples in slices is a numpy array thing)\n",
    "labels = array[:,-1]\n",
    "validation_size = 0.10\n",
    "seed = 7\n",
    "feats_train, feats_validation, labels_train, labels_validation = model_selection.train_test_split(features, labels, test_size=validation_size, random_state=seed)\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Training and testing each model using 10-fold cross-validation...\n",
      "LR:\t0.6947\t(0.0342)\n",
      "LDA:\t0.7097\t(0.0478)\n",
      "KNN:\t0.6321\t(0.0365)\n",
      "CART:\t0.6356\t(0.0802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maste\\envs\\envtest\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:\t0.6395\t(0.0622)\n",
      "SVM:\t0.6246\t(0.0355)\n",
      "RF:\t0.7209\t(0.0355)\n"
     ]
    }
   ],
   "source": [
    "#learn!\n",
    "print('Initializing models...')\n",
    "# Spot Check Algorithms\n",
    "models = [('LR', LogisticRegression()),\n",
    "          ('LDA', LinearDiscriminantAnalysis()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('CART', DecisionTreeClassifier()),\n",
    "          ('NB', GaussianNB()),\n",
    "          ('SVM', SVC()),\n",
    "          ('RF',RandomForestClassifier())]\n",
    "print('Training and testing each model using 10-fold cross-validation...')\n",
    "# https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, feats_train, labels_train,\n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '{}:\\t{:.4f}\\t({:.4f})'.format(name, cv_results.mean(),\n",
    "                                         cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ae33532e76c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcm_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1 2 3 4 5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "cm_labels = '1 2 3 4 5'\n",
    "#print(confusion_matrix(labels_validation, predictions, labels=cm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directory = \\'C:\\\\Users\\\\maste\\\\Desktop\\\\RAFT\\\\arabicScraperOutput\\\\frequency_list\\'\\nmissingList = {}\\nfor filename in os.listdir(directory):\\nif filename.endswith(\\'.txt\\'):\\n    myFile = os.path.join(directory, filename)\\n    f = open(myFile, \"r\", encoding=\"utf8\")\\n    text = f.read()\\n    f.close()\\n    dataList = text.split(\\'\\n\\')\\n    total = 0\\n    missing = 0\\n    for line in dataList:\\n        if line != \\'\\':\\n            total += 1\\n            d = {}\\n            d = line.split(\":::\")\\n            #print(line)\\n            #print(d)\\n            pos = d[3]\\n            lemma = standardizeWord(d[1])\\n            if pos == \\'conj_sub\\':\\n                pos = \\'conj\\'\\n            if pos == \\'part_verb\\':\\n                pos = \\'part\\'\\n            if lemma == \\'هل\\' and pos == \\'part_interrog\\':\\n                pos = \\'prep\\'\\n            if lemma == \\'منذ\\' and pos == \\'conj\\':\\n                pos = \\'prep\\'\\n            if lemma == \\'عند\\' and pos == \\'noun\\':\\n                pos = \\'prep\\'\\n            if pos != \\'digit\\' and pos != \\'noun_prop\\':\\n                key = lemma+\":::\"+pos\\n                if (key not in frequencies):\\n                    checkMissing = True\\n                    if pos == \\'adj\\':\\n                        test = lemma+\":::noun\"\\n                        if test in frequencies:\\n                            checkMissing = False\\n                    missing += 1\\n                    if checkMissing == True:\\n                        if key in missingList:\\n                            missingList[key] += 1\\n                        else:\\n                            missingList[key] = 1\\n                            #print(key,\" does not exists\")\\n                #print()\\n    #print(missing,\"/\",total,\" or \",missing/total,\"%\")\\ns = pd.Series(missingList, dtype=\\'int\\')\\ns = s.sort_values(ascending=False)\\n#print(s)\\n   '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''directory = 'C:\\\\Users\\\\maste\\\\Desktop\\\\RAFT\\\\arabicScraperOutput\\\\frequency_list'\n",
    "missingList = {}\n",
    "for filename in os.listdir(directory):\n",
    "if filename.endswith('.txt'):\n",
    "    myFile = os.path.join(directory, filename)\n",
    "    f = open(myFile, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    dataList = text.split('\\n')\n",
    "    total = 0\n",
    "    missing = 0\n",
    "    for line in dataList:\n",
    "        if line != '':\n",
    "            total += 1\n",
    "            d = {}\n",
    "            d = line.split(\":::\")\n",
    "            #print(line)\n",
    "            #print(d)\n",
    "            pos = d[3]\n",
    "            lemma = standardizeWord(d[1])\n",
    "            if pos == 'conj_sub':\n",
    "                pos = 'conj'\n",
    "            if pos == 'part_verb':\n",
    "                pos = 'part'\n",
    "            if lemma == 'هل' and pos == 'part_interrog':\n",
    "                pos = 'prep'\n",
    "            if lemma == 'منذ' and pos == 'conj':\n",
    "                pos = 'prep'\n",
    "            if lemma == 'عند' and pos == 'noun':\n",
    "                pos = 'prep'\n",
    "            if pos != 'digit' and pos != 'noun_prop':\n",
    "                key = lemma+\":::\"+pos\n",
    "                if (key not in frequencies):\n",
    "                    checkMissing = True\n",
    "                    if pos == 'adj':\n",
    "                        test = lemma+\":::noun\"\n",
    "                        if test in frequencies:\n",
    "                            checkMissing = False\n",
    "                    missing += 1\n",
    "                    if checkMissing == True:\n",
    "                        if key in missingList:\n",
    "                            missingList[key] += 1\n",
    "                        else:\n",
    "                            missingList[key] = 1\n",
    "                            #print(key,\" does not exists\")\n",
    "                #print()\n",
    "    #print(missing,\"/\",total,\" or \",missing/total,\"%\")\n",
    "s = pd.Series(missingList, dtype='int')\n",
    "s = s.sort_values(ascending=False)\n",
    "#print(s)\n",
    "   '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
