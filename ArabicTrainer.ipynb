{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import model classes\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word count  sentence count    p95       mean  median      noun      verb  \\\n",
      "0            40               7  21.70  11.333333    10.0  0.676471  0.000000   \n",
      "1           247              26  76.60  22.250000     9.5  0.556180  0.112360   \n",
      "2            20               2   9.20   3.200000     1.0  0.687500  0.125000   \n",
      "3            42               3  17.50   5.666667     3.0  0.647059  0.117647   \n",
      "4            29               4   7.50   2.666667     1.0  0.562500  0.187500   \n",
      "5            26               2  11.00   4.200000     3.0  0.619048  0.047619   \n",
      "6            23               3   8.40   4.250000     3.5  0.529412  0.294118   \n",
      "7           101              20  47.00  13.666667     3.5  0.707317  0.060976   \n",
      "8            86              22  38.65  15.250000     8.5  0.704918  0.049180   \n",
      "9            95               6  34.25  11.500000     6.0  0.608696  0.159420   \n",
      "10           74              18  29.00   9.333333     4.0  0.625000  0.125000   \n",
      "11           18               4   6.75   2.666667     1.5  0.500000  0.187500   \n",
      "12           15               2   4.70   2.500000     2.0  0.500000  0.100000   \n",
      "13          127               9  37.10  12.000000     7.0  0.510417  0.156250   \n",
      "14          158              12  44.45  15.125000    10.0  0.462810  0.132231   \n",
      "15          118               8  39.50  13.833333    10.0  0.590361  0.132530   \n",
      "16           72               6  19.70   7.428571     5.0  0.442308  0.230769   \n",
      "17          106               6  35.75  13.666667    11.0  0.524390  0.146341   \n",
      "18          150               6  48.75  19.000000    16.0  0.517544  0.157895   \n",
      "19           93              13  32.80  13.200000     8.0  0.560606  0.121212   \n",
      "20           17               3   6.80   2.800000     2.0  0.571429  0.142857   \n",
      "21          151               9  42.10  13.500000     5.0  0.500000  0.185185   \n",
      "22          109              11  29.30   9.714286     6.0  0.514706  0.088235   \n",
      "23          105               9  39.50  12.857143     9.0  0.555556  0.100000   \n",
      "24          113              10  28.90   9.375000     5.0  0.506667  0.133333   \n",
      "25           84               6  26.90   8.571429     6.0  0.583333  0.100000   \n",
      "26          102               7  29.55   9.750000     6.5  0.500000  0.141026   \n",
      "27          128               8  44.10  13.571429     8.0  0.600000  0.084211   \n",
      "28           92               8  28.90  10.000000     6.0  0.528571  0.142857   \n",
      "29          118               6  33.80  10.625000     6.5  0.529412  0.152941   \n",
      "..          ...             ...    ...        ...     ...       ...       ...   \n",
      "858         141              28  39.00  10.000000     3.0  0.572727  0.136364   \n",
      "859         294              22  88.00  24.888889    12.0  0.553571  0.151786   \n",
      "860         143               9  49.75  17.666667     9.5  0.566038  0.179245   \n",
      "861         109              11  27.70  10.571429    10.0  0.459459  0.135135   \n",
      "862          63              11  29.80  11.500000     5.5  0.739130  0.130435   \n",
      "863          81              23  39.55  12.750000     2.0  0.901961  0.058824   \n",
      "864         104              12  22.25   9.500000     6.5  0.438596  0.105263   \n",
      "865         182              47  46.80  15.000000     6.0  0.746667  0.026667   \n",
      "866         144              14  40.20  13.714286    10.0  0.531250  0.145833   \n",
      "867         191              19  48.60  15.111111     6.0  0.477941  0.169118   \n",
      "868          10               4   2.90   2.000000     2.0  0.750000  0.000000   \n",
      "869          50              10  16.20   6.750000     4.0  0.666667  0.037037   \n",
      "870         151              14  44.00  14.571429     9.0  0.549020  0.156863   \n",
      "871         233              24  73.90  23.285714    12.0  0.558282  0.122699   \n",
      "872         115              21  28.40   9.875000     6.0  0.430380  0.139241   \n",
      "873          53              18  23.85  10.000000     5.5  0.675000  0.125000   \n",
      "874         159              14  43.00  12.777778     6.0  0.513043  0.165217   \n",
      "875         123              26  35.00  10.285714     2.0  0.611111  0.111111   \n",
      "876          99              12  36.00  10.571429     7.0  0.648649  0.094595   \n",
      "877         123              24  30.45   8.000000     2.5  0.656250  0.031250   \n",
      "878         199              36  60.25  16.250000     5.5  0.638462  0.084615   \n",
      "879          75              16  22.20   6.142857     2.0  0.697674  0.069767   \n",
      "880           3               3   1.00   1.000000     1.0  1.000000  0.000000   \n",
      "881          12               3   4.40   2.000000     1.0  0.625000  0.000000   \n",
      "882          22               5   9.20   4.333333     2.0  0.769231  0.000000   \n",
      "883         157              10  45.85  14.125000     9.0  0.557522  0.123894   \n",
      "884         110              23  23.00   9.111111     7.0  0.378049  0.134146   \n",
      "885         231              32  55.50  15.727273     7.0  0.398844  0.242775   \n",
      "886         233              29  63.70  22.000000    13.0  0.437500  0.153409   \n",
      "887         326              30  88.40  25.444444     9.0  0.532751  0.165939   \n",
      "\n",
      "         prep      part      conj       adv       adj  difficulty  \n",
      "0    0.294118  0.000000  0.000000  0.000000  0.029412           1  \n",
      "1    0.196629  0.000000  0.005618  0.011236  0.089888           1  \n",
      "2    0.062500  0.000000  0.000000  0.000000  0.062500           1  \n",
      "3    0.088235  0.000000  0.029412  0.000000  0.088235           1  \n",
      "4    0.062500  0.000000  0.062500  0.000000  0.062500           1  \n",
      "5    0.142857  0.000000  0.000000  0.000000  0.142857           1  \n",
      "6    0.117647  0.000000  0.000000  0.000000  0.058824           1  \n",
      "7    0.170732  0.000000  0.024390  0.012195  0.024390           1  \n",
      "8    0.229508  0.000000  0.000000  0.000000  0.016393           1  \n",
      "9    0.115942  0.000000  0.043478  0.000000  0.057971           1  \n",
      "10   0.196429  0.000000  0.017857  0.017857  0.000000           1  \n",
      "11   0.062500  0.000000  0.062500  0.062500  0.125000           1  \n",
      "12   0.300000  0.000000  0.000000  0.000000  0.000000           1  \n",
      "13   0.156250  0.000000  0.010417  0.010417  0.125000           2  \n",
      "14   0.190083  0.000000  0.049587  0.008264  0.115702           2  \n",
      "15   0.108434  0.012048  0.024096  0.000000  0.132530           2  \n",
      "16   0.134615  0.000000  0.057692  0.000000  0.096154           2  \n",
      "17   0.170732  0.000000  0.000000  0.000000  0.121951           2  \n",
      "18   0.131579  0.008772  0.035088  0.000000  0.149123           2  \n",
      "19   0.242424  0.015152  0.000000  0.000000  0.060606           2  \n",
      "20   0.071429  0.000000  0.000000  0.071429  0.142857           2  \n",
      "21   0.157407  0.000000  0.027778  0.009259  0.037037           2  \n",
      "22   0.235294  0.014706  0.000000  0.000000  0.102941           2  \n",
      "23   0.166667  0.011111  0.033333  0.000000  0.111111           2  \n",
      "24   0.093333  0.000000  0.026667  0.000000  0.160000           2  \n",
      "25   0.133333  0.000000  0.033333  0.000000  0.116667           2  \n",
      "26   0.153846  0.000000  0.076923  0.000000  0.089744           2  \n",
      "27   0.105263  0.010526  0.010526  0.000000  0.147368           2  \n",
      "28   0.142857  0.014286  0.042857  0.000000  0.085714           2  \n",
      "29   0.094118  0.023529  0.058824  0.000000  0.094118           2  \n",
      "..        ...       ...       ...       ...       ...         ...  \n",
      "858  0.109091  0.009091  0.009091  0.009091  0.063636           2  \n",
      "859  0.138393  0.004464  0.053571  0.013393  0.062500           2  \n",
      "860  0.132075  0.000000  0.000000  0.000000  0.047170           2  \n",
      "861  0.175676  0.000000  0.135135  0.027027  0.054054           2  \n",
      "862  0.108696  0.000000  0.000000  0.000000  0.021739           2  \n",
      "863  0.019608  0.000000  0.000000  0.000000  0.019608           2  \n",
      "864  0.245614  0.000000  0.070175  0.000000  0.122807           2  \n",
      "865  0.080000  0.000000  0.013333  0.000000  0.133333           2  \n",
      "866  0.156250  0.000000  0.031250  0.000000  0.104167           2  \n",
      "867  0.176471  0.000000  0.014706  0.014706  0.088235           2  \n",
      "868  0.250000  0.000000  0.000000  0.000000  0.000000           2  \n",
      "869  0.222222  0.000000  0.000000  0.000000  0.074074           2  \n",
      "870  0.137255  0.000000  0.019608  0.000000  0.088235           2  \n",
      "871  0.208589  0.000000  0.000000  0.012270  0.073620           2  \n",
      "872  0.227848  0.012658  0.012658  0.012658  0.151899           2  \n",
      "873  0.050000  0.000000  0.000000  0.000000  0.150000           2  \n",
      "874  0.095652  0.000000  0.052174  0.008696  0.104348           2  \n",
      "875  0.194444  0.000000  0.000000  0.000000  0.027778           2  \n",
      "876  0.094595  0.000000  0.027027  0.000000  0.108108           2  \n",
      "877  0.140625  0.000000  0.046875  0.000000  0.062500           2  \n",
      "878  0.138462  0.000000  0.000000  0.023077  0.061538           2  \n",
      "879  0.093023  0.000000  0.046512  0.000000  0.046512           2  \n",
      "880  0.000000  0.000000  0.000000  0.000000  0.000000           2  \n",
      "881  0.125000  0.000000  0.000000  0.000000  0.125000           2  \n",
      "882  0.153846  0.000000  0.000000  0.000000  0.076923           2  \n",
      "883  0.123894  0.000000  0.044248  0.017699  0.115044           2  \n",
      "884  0.097561  0.000000  0.073171  0.024390  0.134146           2  \n",
      "885  0.156069  0.005780  0.075145  0.011561  0.052023           2  \n",
      "886  0.221591  0.000000  0.022727  0.011364  0.090909           3  \n",
      "887  0.117904  0.004367  0.039301  0.000000  0.087336           3  \n",
      "\n",
      "[888 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "filename = 'arabicReading.data'\n",
    "names = ['word count','sentence count','p95','mean','median','noun','verb','prep','part','conj','adv','adj','difficulty']\n",
    "with open(filename) as data_file:\n",
    "    dataset = pandas.read_csv(data_file,names=names)  # pandas DataFrame\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataset.values\n",
    "len_names = len(names)\n",
    "features = array[:,0:len_names-1]  # comma in slice signifies a tuple (tuples in slices is a numpy array thing)\n",
    "labels = array[:,-1]\n",
    "validation_size = 0.10\n",
    "seed = 7\n",
    "feats_train, feats_validation, labels_train, labels_validation = model_selection.train_test_split(features, labels, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7  # seeds the randomizer so that 'random' choices are the same in each run\n",
    "scoring = 'accuracy'\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Training and testing each model using 10-fold cross-validation...\n",
      "LR:\t0.6634\t(0.0411)\n",
      "LDA:\t0.6684\t(0.0495)\n",
      "KNN:\t0.6395\t(0.0545)\n",
      "CART:\t0.6133\t(0.0494)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maste\\envs\\envtest\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:\t0.6032\t(0.0585)\n",
      "SVM:\t0.6195\t(0.0310)\n"
     ]
    }
   ],
   "source": [
    "print('Initializing models...')\n",
    "# Spot Check Algorithms\n",
    "models = [('LR', LogisticRegression()),\n",
    "          ('LDA', LinearDiscriminantAnalysis()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('CART', DecisionTreeClassifier()),\n",
    "          ('NB', GaussianNB()),\n",
    "          ('SVM', SVC())]\n",
    "print('Training and testing each model using 10-fold cross-validation...')\n",
    "# https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, feats_train, labels_train,\n",
    "                                                 cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '{}:\\t{:.4f}\\t({:.4f})'.format(name, cv_results.mean(),\n",
    "                                         cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing algorithm comparison boxplots...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Drawing algorithm comparison boxplots...')\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig = plt.gcf()\n",
    "fig.savefig('compare_algorithms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6067415730337079\n",
      "\n",
      "Confusion matrix:\n",
      "labels: ['1', '2', '3', '4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maste\\envs\\envtest\\lib\\site-packages\\sklearn\\metrics\\classification.py:258: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.all([l not in y_true for l in labels]):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one label specified must be in y_true",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-30c6439ec513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcm_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1 2 3 4'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labels:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classification report:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maste\\envs\\envtest\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one label specified must be in y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: At least one label specified must be in y_true"
     ]
    }
   ],
   "source": [
    "# Make predictions on validation dataset\n",
    "# Using svc because it performed best on cross-validation\n",
    "final_model = SVC()\n",
    "final_model.fit(feats_train, labels_train)\n",
    "predictions = final_model.predict(feats_validation)\n",
    "print('Accuracy:', accuracy_score(labels_validation, predictions))\n",
    "print()\n",
    "print('Confusion matrix:')\n",
    "cm_labels = '1 2 3 4'.split()\n",
    "print('labels:', cm_labels)\n",
    "print(confusion_matrix(labels_validation, predictions, labels=cm_labels))\n",
    "print()\n",
    "print('Classification report:')\n",
    "print(classification_report(labels_validation, predictions))\n",
    "print(dir(final_model))\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    print('Feature \"importances\":')\n",
    "    print(final_model.feature_importances_)\n",
    "if hasattr(final_model, 'coef_'):\n",
    "    print('Feature correlation coefficients:')\n",
    "    print(final_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
